{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetSeqGenerator(Sequence):\n",
    "  \n",
    "    def __init__(self, data, labels, batch_size, normalize=False):\n",
    "      self.data = data\n",
    "      self.labels = labels\n",
    "      if len(self.data) != len(self.labels):\n",
    "        raise ValueError(\"Data and Labels have to be same size\")\n",
    "      self.batch_size = batch_size\n",
    "      self.normalize = normalize\n",
    "      \n",
    "      # If normalize, then normalize each timeseries to sum to 1 on each keyword\n",
    "      if self.normalize:\n",
    "        for i in range(len(self.data)):\n",
    "            # Normalizes the rows of the matrix to 1\n",
    "            # Axis=0 should normalize the columns to 1\n",
    "            self.data[i] = self.data[i]/np.max(self.data[i], axis=1)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Go through each set of files and augment them as needed\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        timeseries = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        return timeseries, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the list of paths, as the number of events is not known\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "            self.data = shuffle(self.data, self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(category, length, limit=None):\n",
    "  \"\"\"\n",
    "  Generates timeseries from the tweets, and gets the next event to occur in the future\n",
    "  \"\"\"\n",
    "  return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "  \"\"\"\n",
    "  Cleans a tweet to just English characters, removing URLs, etc.\n",
    "  What is given is just the tweet itself, not the metadata\n",
    "  \"\"\"\n",
    "  tweet = re.sub(r\"http\\S+\", \"\", tweet) # removes URLs\n",
    "  tweet = re.sub(r\"[^a-zA-Z0-9]+\", ' ', tweet) # Removes non-alphanumeric chars\n",
    "  tweet = tweet.lower() # Lowercases it\n",
    "  \n",
    "  return tweet\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    SELECT created_at,keywords FROM `hdc-politie-team-3.politie.tweets` WHERE iso_language_code='nl' and created_at BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY created_at ASC\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "  SELECT * FROM `hdc-politie-team-3.politie.meldkamer` WHERE start_incident BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY start_incident ASC\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd\n",
    "import google.datalab.bigquery as bq\n",
    "    \n",
    "def getDATA():\n",
    "    query_K2019tweets = \"\"\"\n",
    "\n",
    "    SELECT created_at,keywords FROM `hdc-politie-team-3.politie.tweets` WHERE iso_language_code='nl' and created_at BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY created_at ASC\n",
    "\n",
    "    \"\"\"\n",
    "    print(query_K2019tweets)\n",
    "    # Transform your query to BigQuery object\n",
    "    bq_object = bq.Query(query_K2019tweets)\n",
    "\n",
    "    # Execute your query\n",
    "    result = bq_object.execute().result()\n",
    "\n",
    "    # Transform your output to a Pandas dataframe\n",
    "    kings2019tweets_df = result.to_dataframe()\n",
    "    \n",
    "    return kings2019tweets_df\n",
    "  \n",
    "\n",
    "def getLabels():\n",
    "  query_K2019tweets = \"\"\"\n",
    "\n",
    "  SELECT * FROM `hdc-politie-team-3.politie.meldkamer` WHERE start_incident BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY start_incident ASC\n",
    "\n",
    "  \"\"\"\n",
    "  print(query_K2019tweets)\n",
    "  # Transform your query to BigQuery object\n",
    "  bq_object = bq.Query(query_K2019tweets)\n",
    "\n",
    "  # Execute your query\n",
    "  result = bq_object.execute().result()\n",
    "\n",
    "  # Transform your output to a Pandas dataframe\n",
    "  kings2019mcl_df = result.to_dataframe()\n",
    "\n",
    "  return kings2019mcl_df\n",
    "\n",
    "\n",
    "def getkeywords():\n",
    "    query_K2019keys = \"\"\"\n",
    "\n",
    "   SELECT distinct key FROM `hdc-politie-team-3.politie.tweets`,unnest(keywords) as key\n",
    "\n",
    "    \"\"\"\n",
    "    # Transform your query to BigQuery object\n",
    "    bq_object = bq.Query(query_K2019keys)\n",
    "    \n",
    "    # Execute your query\n",
    "    result = bq_object.execute().result()\n",
    "\n",
    "    # Transform your output to a Pandas dataframe\n",
    "    kings2019keys_df = result.to_dataframe()\n",
    "    aj=kings2019keys_df.values.tolist()\n",
    "    newa=[]\n",
    "    for i in range(len(aj)):\n",
    "        newa.append(aj[i][0])\n",
    "    \n",
    "    nexta=sorted(newa)\n",
    "    \n",
    "    return nexta\n",
    "  \n",
    "def getvectorfortime(b,a):\n",
    "    newb=b.values.tolist()\n",
    "    c=[0]*len(a)\n",
    "    for i in range (len(newb)):\n",
    "        am= newb[i][1]\n",
    "        \n",
    "        for j in range (len(am)):\n",
    "            c[a.index(am[j])]+=1\n",
    "    return c\n",
    "        \n",
    "\n",
    "keywords=getkeywords()\n",
    "data=getDATA()\n",
    "labels=getLabels()\n",
    "\n",
    "def create_timestep(start_time, step_size, length, incident_type='Brand'):\n",
    "  \"\"\"\n",
    "  \n",
    "  step_size has to be a pd.timeDelta to work, otherwise it will fail\n",
    "  \n",
    "  Returns the timeseries and the labels for the timeseries\n",
    "  \n",
    "  \"\"\"\n",
    "  timeseries = []\n",
    "  for i in range(length):\n",
    "    mask = (data['created_at'] >= start_time+i*step_size) & (data['created_at'] <= start_time+(i+1)*step_size)\n",
    "    sub_data = data[mask]\n",
    "    timeseries.append(getvectorfortime(sub_data,keywords))\n",
    "  \n",
    "  timeseries = np.asarray(timeseries)\n",
    "  end_time = start_time+length*step_size\n",
    "  # 1min, 5min, 10min, 15min, 30min\n",
    "  time_periods = [pd.Timedelta(\"1 minute\"), pd.Timedelta(\"5 minute\"), pd.Timedelta(\"10 minute\"), pd.Timedelta(\"15 minute\"), pd.Timedelta(\"30 minute\")]\n",
    "  time_label = []\n",
    "  for period in time_periods:\n",
    "    label_mask = (labels['start_incident'] >= (end_time)) & (labels['start_incident'] <= (end_time+period)) & (labels['mcl'] == incident_type)\n",
    "    if len(labels[label_mask]) > 0:\n",
    "      time_label.append(1)\n",
    "    else:\n",
    "      time_label.append(0)\n",
    "    \n",
    "  return timeseries, time_label\n",
    "  \n",
    "\n",
    "\n",
    "class TweetSeqGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, data, labels, batch_size, normalize=False):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        if len(self.data) != len(self.labels):\n",
    "            raise ValueError(\"Data and Labels have to be same size\")\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # If normalize, then normalize each timeseries to sum to 1 on each keyword\n",
    "        if self.normalize:\n",
    "            for i in range(len(self.data)):\n",
    "                # Normalizes the rows of the matrix to 1\n",
    "                # Axis=0 should normalize the columns to 1\n",
    "                self.data[i] = self.data[i]/np.max(self.data[i], axis=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Go through each set of files and augment them as needed\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        timeseries = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        return timeseries, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the list of paths, as the number of events is not known\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.data, self.labels = shuffle(self.data, self.labels)\n",
    "\n",
    "\n",
    "def generate_timeseries(category, length, limit=None):\n",
    "    \"\"\"\n",
    "    Generates timeseries from the tweets, and gets the next event to occur in the future\n",
    "    \"\"\"\n",
    "    return NotImplementedError\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Cleans a tweet to just English characters, removing URLs, etc.\n",
    "    What is given is just the tweet itself, not the metadata\n",
    "    \"\"\"\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet) # removes URLs\n",
    "    tweet = re.sub(r\"[^a-zA-Z0-9]+\", ' ', tweet) # Removes non-alphanumeric chars\n",
    "    tweet = tweet.lower() # Lowercases it\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "step_size = pd.Timedelta(\"1 minute\")\n",
    "start_time = pd.to_datetime(\"2019-04-26 00:00:00\")\n",
    "length = 30\n",
    "tweet_keywords = []\n",
    "melding = []\n",
    "for i in range(1000):\n",
    "  timeseries, label = create_timestep(start_time, step_size, length=length)\n",
    "  tweet_keywords.append(timeseries)\n",
    "  melding.append(label)\n",
    "  start_time += step_size * (length/15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Hsape: (750, 30, 1100)\n",
      "Y Shape: (750, 5)\n"
     ]
    }
   ],
   "source": [
    "maxlen = length\n",
    "sequences = tweet_keywords\n",
    "next_melding = melding\n",
    "\n",
    "x = np.asarray(sequences)\n",
    "y = np.asarray(next_melding)\n",
    "\n",
    "x, y = shuffle(x,y)\n",
    "\n",
    "validation_cut = int(0.25*len(x))\n",
    "x1 = x[:validation_cut]\n",
    "y1 = y[:validation_cut]\n",
    "\n",
    "x = x[validation_cut:]\n",
    "y = y[validation_cut:]\n",
    "\n",
    "print(\"X Hsape: {}\".format(x.shape))\n",
    "print(\"Y Shape: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(tweet_keywords[0][0]))))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(melding[0])))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "train_gen = TweetSeqGenerator(data=x, labels=y, batch_size=32)\n",
    "val_gen = TweetSeqGenerator(data=x1, labels=y1, batch_size=32)\n",
    "\n",
    "model.fit_generator(generator=train_gen, validation_data=val_gen, use_multiprocessing=True, workers=4, epochs=1000)\n",
    "\n",
    "\n",
    "def predict_melding():\n",
    "    for i in range(0,len(sequences)-maxlen):\n",
    "        x_pred = np.array(sequences[i:i+maxlen])\n",
    "        pred = model.predict(x_pred)\n",
    "        print()\n",
    "        for j in range(len(pred)):\n",
    "            print(\"time = \" + str(i) + \" predictions for time = \"+str(i+j+1)+ \":\")\n",
    "            for k in range(len(pred[0])):\n",
    "                print(\"indicent \" + str(k) + \": \" + str(round(pred[0][k]* 100,2)) + \"%\")\n",
    "\n",
    "predict_melding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
