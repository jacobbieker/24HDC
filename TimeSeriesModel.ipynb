{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetSeqGenerator(Sequence):\n",
    "  \n",
    "    def __init__(self, data, labels, batch_size, normalize=False):\n",
    "      self.data = data\n",
    "      self.labels = labels\n",
    "      if len(self.data) != len(self.labels):\n",
    "        raise ValueError(\"Data and Labels have to be same size\")\n",
    "      self.batch_size = batch_size\n",
    "      self.normalize = normalize\n",
    "      \n",
    "      # If normalize, then normalize each timeseries to sum to 1 on each keyword\n",
    "      if self.normalize:\n",
    "        for i in range(len(self.data)):\n",
    "            # Normalizes the rows of the matrix to 1\n",
    "            # Axis=0 should normalize the columns to 1\n",
    "            self.data[i] = self.data[i]/np.max(self.data[i], axis=1)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Go through each set of files and augment them as needed\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        timeseries = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        return timeseries, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the list of paths, as the number of events is not known\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "            self.data = shuffle(self.data, self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(category, length, limit=None):\n",
    "  \"\"\"\n",
    "  Generates timeseries from the tweets, and gets the next event to occur in the future\n",
    "  \"\"\"\n",
    "  return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "  \"\"\"\n",
    "  Cleans a tweet to just English characters, removing URLs, etc.\n",
    "  What is given is just the tweet itself, not the metadata\n",
    "  \"\"\"\n",
    "  tweet = re.sub(r\"http\\S+\", \"\", tweet) # removes URLs\n",
    "  tweet = re.sub(r\"[^a-zA-Z0-9]+\", ' ', tweet) # Removes non-alphanumeric chars\n",
    "  tweet = tweet.lower() # Lowercases it\n",
    "  \n",
    "  return tweet\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    SELECT created_at,keywords FROM `hdc-politie-team-3.politie.tweets` WHERE iso_language_code='nl' and created_at BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY created_at ASC\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "  SELECT * FROM `hdc-politie-team-3.politie.meldkamer` WHERE start_incident BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY start_incident ASC\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd\n",
    "import google.datalab.bigquery as bq\n",
    "    \n",
    "def getDATA():\n",
    "    query_K2019tweets = \"\"\"\n",
    "\n",
    "    SELECT created_at,keywords FROM `hdc-politie-team-3.politie.tweets` WHERE iso_language_code='nl' and created_at BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY created_at ASC\n",
    "\n",
    "    \"\"\"\n",
    "    print(query_K2019tweets)\n",
    "    # Transform your query to BigQuery object\n",
    "    bq_object = bq.Query(query_K2019tweets)\n",
    "\n",
    "    # Execute your query\n",
    "    result = bq_object.execute().result()\n",
    "\n",
    "    # Transform your output to a Pandas dataframe\n",
    "    kings2019tweets_df = result.to_dataframe()\n",
    "    \n",
    "    return kings2019tweets_df\n",
    "  \n",
    "\n",
    "def getLabels():\n",
    "  query_K2019tweets = \"\"\"\n",
    "\n",
    "  SELECT * FROM `hdc-politie-team-3.politie.meldkamer` WHERE start_incident BETWEEN \"2019-04-26\" AND \"2019-04-28\" ORDER BY start_incident ASC\n",
    "\n",
    "  \"\"\"\n",
    "  print(query_K2019tweets)\n",
    "  # Transform your query to BigQuery object\n",
    "  bq_object = bq.Query(query_K2019tweets)\n",
    "\n",
    "  # Execute your query\n",
    "  result = bq_object.execute().result()\n",
    "\n",
    "  # Transform your output to a Pandas dataframe\n",
    "  kings2019mcl_df = result.to_dataframe()\n",
    "\n",
    "  return kings2019mcl_df\n",
    "\n",
    "\n",
    "def getkeywords():\n",
    "    query_K2019keys = \"\"\"\n",
    "\n",
    "   SELECT distinct key FROM `hdc-politie-team-3.politie.tweets`,unnest(keywords) as key\n",
    "\n",
    "    \"\"\"\n",
    "    # Transform your query to BigQuery object\n",
    "    bq_object = bq.Query(query_K2019keys)\n",
    "    \n",
    "    # Execute your query\n",
    "    result = bq_object.execute().result()\n",
    "\n",
    "    # Transform your output to a Pandas dataframe\n",
    "    kings2019keys_df = result.to_dataframe()\n",
    "    aj=kings2019keys_df.values.tolist()\n",
    "    newa=[]\n",
    "    for i in range(len(aj)):\n",
    "        newa.append(aj[i][0])\n",
    "    \n",
    "    nexta=sorted(newa)\n",
    "    \n",
    "    return nexta\n",
    "  \n",
    "def getvectorfortime(b,a):\n",
    "    newb=b.values.tolist()\n",
    "    c=[0]*len(a)\n",
    "    for i in range (len(newb)):\n",
    "        am= newb[i][1]\n",
    "        \n",
    "        for j in range (len(am)):\n",
    "            c[a.index(am[j])]+=1\n",
    "    return c\n",
    "        \n",
    "\n",
    "keywords=getkeywords()\n",
    "data=getDATA()\n",
    "labels=getLabels()\n",
    "\n",
    "def create_event_timesteps(start_time, step_size, end_time, length, incident_type='Brand'):\n",
    "  \"\"\"\n",
    "  \n",
    "  step_size has to be a pd.timeDelta to work, otherwise it will fail\n",
    "  \n",
    "  Returns the timeseries and the labels for the timeseries\n",
    "  \n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  events = []\n",
    "  \n",
    "  current_time = start_time + 0*step_size\n",
    "  \n",
    "  while current_time < end_time:\n",
    "    mask = (labels['start_incident'] >= (current_time)) & (labels['start_incident'] <= current_time+step_size) & (labels['mcl'] == incident_type)\n",
    "    tmp = labels[mask]\n",
    "    if len(tmp) > 0:\n",
    "      events.append(current_time)\n",
    "    current_time += step_size\n",
    "    \n",
    "  \n",
    "  dataset = []\n",
    "  labels = []\n",
    "  for event in events:\n",
    "    for j in range(length+2):\n",
    "      timeseries = []\n",
    "      start_time = event - (j + 1)*step_size\n",
    "      for i in range(length):\n",
    "        mask = (data['created_at'] >= start_time+i*step_size) & (data['created_at'] <= start_time+(i+1)*step_size)\n",
    "        sub_data = data[mask]\n",
    "        timeseries.append(getvectorfortime(sub_data,keywords))\n",
    "\n",
    "      timeseries = np.asarray(timeseries)\n",
    "      end_time = start_time+length*step_size\n",
    "      # 1min, 5min, 10min, 15min, 30min\n",
    "      time_periods = [pd.Timedelta(\"1 minute\"), pd.Timedelta(\"5 minute\"), pd.Timedelta(\"10 minute\"), pd.Timedelta(\"15 minute\"), pd.Timedelta(\"30 minute\")]\n",
    "      time_label = []\n",
    "      for period in time_periods:\n",
    "        label_mask = (labels['start_incident'] >= (end_time)) & (labels['start_incident'] <= (end_time+period)) & (labels['mcl'] == incident_type)\n",
    "        if len(labels[label_mask]) > 0:\n",
    "          time_label.append(1)\n",
    "        else:\n",
    "          time_label.append(0)\n",
    "    dataset.append(timeseries)\n",
    "    labels.append(time_label)\n",
    "\n",
    "  dataset = np.asarray(dataset)\n",
    "  labels = np.asarray(labels)\n",
    "    \n",
    "  return dataset, labels\n",
    "\n",
    "def create_timestep(start_time, step_size, length, incident_type='Brand'):\n",
    "  \"\"\"\n",
    "  \n",
    "  step_size has to be a pd.timeDelta to work, otherwise it will fail\n",
    "  \n",
    "  Returns the timeseries and the labels for the timeseries\n",
    "  \n",
    "  \"\"\"\n",
    "  timeseries = []\n",
    "  for i in range(length):\n",
    "    mask = (data['created_at'] >= start_time+i*step_size) & (data['created_at'] <= start_time+(i+1)*step_size)\n",
    "    sub_data = data[mask]\n",
    "    timeseries.append(getvectorfortime(sub_data,keywords))\n",
    "  \n",
    "  timeseries = np.asarray(timeseries)\n",
    "  end_time = start_time+length*step_size\n",
    "  # 1min, 5min, 10min, 15min, 30min\n",
    "  time_periods = [pd.Timedelta(\"1 minute\"), pd.Timedelta(\"5 minute\"), pd.Timedelta(\"10 minute\"), pd.Timedelta(\"15 minute\"), pd.Timedelta(\"30 minute\")]\n",
    "  time_label = []\n",
    "  for period in time_periods:\n",
    "    label_mask = (labels['start_incident'] >= (end_time)) & (labels['start_incident'] <= (end_time+period)) & (labels['mcl'] == incident_type)\n",
    "    if len(labels[label_mask]) > 0:\n",
    "      time_label.append(1)\n",
    "    else:\n",
    "      time_label.append(0)\n",
    "    \n",
    "  return timeseries, time_label\n",
    "  \n",
    "\n",
    "\n",
    "class TweetSeqGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, data, labels, batch_size, normalize=False):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        if len(self.data) != len(self.labels):\n",
    "            raise ValueError(\"Data and Labels have to be same size\")\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # If normalize, then normalize each timeseries to sum to 1 on each keyword\n",
    "        if self.normalize:\n",
    "            for i in range(len(self.data)):\n",
    "                # Normalizes the rows of the matrix to 1\n",
    "                # Axis=0 should normalize the columns to 1\n",
    "                self.data[i] = self.data[i]/np.max(self.data[i], axis=1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Go through each set of files and augment them as needed\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        timeseries = self.data[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        return timeseries, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the list of paths, as the number of events is not known\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.data, self.labels = shuffle(self.data, self.labels)\n",
    "\n",
    "\n",
    "def generate_timeseries(category, length, limit=None):\n",
    "    \"\"\"\n",
    "    Generates timeseries from the tweets, and gets the next event to occur in the future\n",
    "    \"\"\"\n",
    "    return NotImplementedError\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Cleans a tweet to just English characters, removing URLs, etc.\n",
    "    What is given is just the tweet itself, not the metadata\n",
    "    \"\"\"\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet) # removes URLs\n",
    "    tweet = re.sub(r\"[^a-zA-Z0-9]+\", ' ', tweet) # Removes non-alphanumeric chars\n",
    "    tweet = tweet.lower() # Lowercases it\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "step_size = pd.Timedelta(\"1 minute\")\n",
    "start_time = pd.to_datetime(\"2019-04-26 00:00:00\")\n",
    "end_time = pd.to_datetime(\"2019-04-29 00:00:00\")\n",
    "length = 30\n",
    "\n",
    "tweet_keywords, melding = create_event_timesteps(start_time, step_size, end_time, length=length)\n",
    "\n",
    "#tweet_keywords = []\n",
    "#melding = []\n",
    "#for i in range(2800):\n",
    "#  timeseries, label = create_timestep(start_time, step_size, length=length)\n",
    "#  tweet_keywords.append(timeseries)\n",
    "#  melding.append(label)\n",
    "#  start_time += step_size * (length/30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Hsape: (2100, 30, 1100)\n",
      "Y Shape: (2100, 5)\n"
     ]
    }
   ],
   "source": [
    "maxlen = length\n",
    "sequences = tweet_keywords\n",
    "next_melding = melding\n",
    "\n",
    "x = np.asarray(sequences)\n",
    "y = np.asarray(next_melding)\n",
    "\n",
    "x, y = shuffle(x,y)\n",
    "\n",
    "validation_cut = int(0.25*len(x))\n",
    "x1 = x[:validation_cut]\n",
    "y1 = y[:validation_cut]\n",
    "\n",
    "x = x[validation_cut:]\n",
    "y = y[validation_cut:]\n",
    "\n",
    "print(\"X Hsape: {}\".format(x.shape))\n",
    "print(\"Y Shape: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "66/66 [==============================] - 14s 217ms/step - loss: 0.3120 - val_loss: 0.3291\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2883 - val_loss: 0.3311\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - 16s 235ms/step - loss: 0.2743 - val_loss: 0.3046\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - 14s 209ms/step - loss: 0.2516 - val_loss: 0.2782\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2374 - val_loss: 0.2721\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - 13s 192ms/step - loss: 0.2373 - val_loss: 0.2750\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2363 - val_loss: 0.2860\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - 13s 191ms/step - loss: 0.2329 - val_loss: 0.2652\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - 13s 192ms/step - loss: 0.2275 - val_loss: 0.2776\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2247 - val_loss: 0.2679\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2253 - val_loss: 0.2667\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - 13s 193ms/step - loss: 0.2237 - val_loss: 0.2828\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2233 - val_loss: 0.2770\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2222 - val_loss: 0.2785\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2229 - val_loss: 0.2699\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - 13s 192ms/step - loss: 0.2212 - val_loss: 0.2715\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2206 - val_loss: 0.2705\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - 13s 192ms/step - loss: 0.2199 - val_loss: 0.2739\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - 13s 193ms/step - loss: 0.2227 - val_loss: 0.2642\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - 13s 192ms/step - loss: 0.2210 - val_loss: 0.2662\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2218 - val_loss: 0.2717\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - 13s 190ms/step - loss: 0.2199 - val_loss: 0.2686\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2207 - val_loss: 0.2716\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - 13s 192ms/step - loss: 0.2196 - val_loss: 0.2770\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2187 - val_loss: 0.2765\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2197 - val_loss: 0.2706\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2186 - val_loss: 0.2726\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - 13s 192ms/step - loss: 0.2190 - val_loss: 0.2733\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2212 - val_loss: 0.2682\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2198 - val_loss: 0.2771\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2193 - val_loss: 0.2815\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2226 - val_loss: 0.2719\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - 13s 193ms/step - loss: 0.2188 - val_loss: 0.2841\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2187 - val_loss: 0.2760\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2183 - val_loss: 0.2768\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2192 - val_loss: 0.2741\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2214 - val_loss: 0.2768\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2199 - val_loss: 0.2794\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2181 - val_loss: 0.2777\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2204 - val_loss: 0.2782\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2199 - val_loss: 0.2790\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2180 - val_loss: 0.2776\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2184 - val_loss: 0.2804\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2216 - val_loss: 0.2768\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2195 - val_loss: 0.2783\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2180 - val_loss: 0.2766\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2204 - val_loss: 0.2788\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - 14s 206ms/step - loss: 0.2181 - val_loss: 0.2755\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2182 - val_loss: 0.2783\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - 13s 202ms/step - loss: 0.2184 - val_loss: 0.2747\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - 13s 202ms/step - loss: 0.2229 - val_loss: 0.2643\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2191 - val_loss: 0.2734\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2216 - val_loss: 0.2787\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2218 - val_loss: 0.2776\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2182 - val_loss: 0.2788\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2181 - val_loss: 0.2790\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2184 - val_loss: 0.2809\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2196 - val_loss: 0.2833\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2220 - val_loss: 0.3165\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - 13s 202ms/step - loss: 0.2221 - val_loss: 0.2937\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2313 - val_loss: 0.2926\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - 13s 201ms/step - loss: 0.2303 - val_loss: 0.2701\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2245 - val_loss: 0.2693\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2259 - val_loss: 0.2753\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2203 - val_loss: 0.2731\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2225 - val_loss: 0.2738\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2195 - val_loss: 0.2769\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2183 - val_loss: 0.2771\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2181 - val_loss: 0.2799\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2180 - val_loss: 0.2817\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2211 - val_loss: 0.2844\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2179 - val_loss: 0.2837\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2179 - val_loss: 0.2846\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2189 - val_loss: 0.2860\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2193 - val_loss: 0.2857\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2180 - val_loss: 0.2851\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2179 - val_loss: 0.2871\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - 13s 194ms/step - loss: 0.2179 - val_loss: 0.2867\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2202 - val_loss: 0.2879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2195 - val_loss: 0.2894\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2179 - val_loss: 0.2894\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2202 - val_loss: 0.2898\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2179 - val_loss: 0.2903\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2204 - val_loss: 0.2901\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - 13s 201ms/step - loss: 0.2188 - val_loss: 0.2903\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2179 - val_loss: 0.2919\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2183 - val_loss: 0.2923\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2180 - val_loss: 0.2927\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2189 - val_loss: 0.2919\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2195 - val_loss: 0.2903\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2186 - val_loss: 0.2898\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2217 - val_loss: 0.2891\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2212 - val_loss: 0.2885\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2224 - val_loss: 0.2890\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - 13s 201ms/step - loss: 0.2202 - val_loss: 0.2895\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2183 - val_loss: 0.2908\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2179 - val_loss: 0.2915\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - 14s 206ms/step - loss: 0.2179 - val_loss: 0.2924\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2227 - val_loss: 0.2905\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2179 - val_loss: 0.2932\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2185 - val_loss: 0.2904\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2196 - val_loss: 0.2878\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2190 - val_loss: 0.2902\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2180 - val_loss: 0.2883\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2202 - val_loss: 0.2908\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2179 - val_loss: 0.2966\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - 13s 202ms/step - loss: 0.2195 - val_loss: 0.2891\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2195 - val_loss: 0.2902\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - 13s 200ms/step - loss: 0.2205 - val_loss: 0.2924\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2184 - val_loss: 0.2789\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2180 - val_loss: 0.2798\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - 13s 198ms/step - loss: 0.2195 - val_loss: 0.2855\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - 13s 196ms/step - loss: 0.2212 - val_loss: 0.2881\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - 13s 199ms/step - loss: 0.2183 - val_loss: 0.2902\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2230 - val_loss: 0.2858\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - 13s 201ms/step - loss: 0.2183 - val_loss: 0.2799\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - 13s 197ms/step - loss: 0.2227 - val_loss: 0.2979\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - 13s 195ms/step - loss: 0.2314 - val_loss: 0.2728\n",
      "Epoch 119/1000\n",
      "19/66 [=======>......................] - ETA: 9s - loss: 0.2546 "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(maxlen, len(tweet_keywords[0][0]))))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(melding[0])))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "train_gen = TweetSeqGenerator(data=x, labels=y, batch_size=32)\n",
    "val_gen = TweetSeqGenerator(data=x1, labels=y1, batch_size=32)\n",
    "\n",
    "model.fit_generator(generator=train_gen, validation_data=val_gen, use_multiprocessing=True, workers=4, epochs=1000)\n",
    "\n",
    "model.save(\"1000_b32_2layer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
